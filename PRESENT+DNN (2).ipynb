{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PRESENT+DNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvgDDeK57YDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Present:\n",
        "\n",
        "        def __init__(self,key,rounds):\n",
        "                \"\"\"Create a PRESENT cipher object\n",
        "\n",
        "                key:    the key as a 128-bit or 80-bit rawstring\n",
        "                rounds: the number of rounds as an integer, 32 by default\n",
        "                \"\"\"\n",
        "                self.rounds = rounds\n",
        "                if len(key) * 8 == 80:\n",
        "                        self.roundkeys = generateRoundkeys80(string2number(key),self.rounds)\n",
        "                elif len(key) * 8 == 128:\n",
        "                        self.roundkeys = generateRoundkeys128(string2number(key),self.rounds)\n",
        "                else:\n",
        "                        raise ValueError( \"Key must be a 128-bit or 80-bit rawstring\")\n",
        "\n",
        "        def encrypt(self,block):\n",
        "                \"\"\"Encrypt 1 block (8 bytes)\n",
        "\n",
        "                Input:  plaintext block as raw string\n",
        "                Output: ciphertext block as raw string\n",
        "                \"\"\"\n",
        "                state = string2number(block)\n",
        "                for i in range (self.rounds-1):\n",
        "                        state = addRoundKey(state,self.roundkeys[i])\n",
        "                        state = sBoxLayer(state)\n",
        "                        state = pLayer(state)\n",
        "                cipher = addRoundKey(state,self.roundkeys[-1])\n",
        "                return number2string_N(cipher,8)\n",
        "\n",
        "        def decrypt(self,block):\n",
        "                \"\"\"Decrypt 1 block (8 bytes)\n",
        "\n",
        "                Input:  ciphertext block as raw string\n",
        "                Output: plaintext block as raw string\n",
        "                \"\"\"\n",
        "                state = string2number(block)\n",
        "                for i in range (self.rounds-1):\n",
        "                        state = addRoundKey(state,self.roundkeys[-i-1])\n",
        "                        state = pLayer_dec(state)\n",
        "                        state = sBoxLayer_dec(state)\n",
        "                decipher = addRoundKey(state,self.roundkeys[0])\n",
        "                return number2string_N(decipher,8)\n",
        "\n",
        "        def get_block_size(self):\n",
        "                return 8\n",
        "\n",
        "#        0   1   2   3   4   5   6   7   8   9   a   b   c   d   e   f\n",
        "Sbox= [0xc,0x5,0x6,0xb,0x9,0x0,0xa,0xd,0x3,0xe,0xf,0x8,0x4,0x7,0x1,0x2]\n",
        "# x 0 1 2 3 4 5 6 7 8 9 A B C D E F\n",
        "# S[x] C 5 6 B 9 0 A D 3 E F 8 4 7 1 2\n",
        "\n",
        "Sbox_inv = [Sbox.index(x) for x in range(16)]\n",
        "PBox = [0,16,32,48,1,17,33,49,2,18,34,50,3,19,35,51,\n",
        "        4,20,36,52,5,21,37,53,6,22,38,54,7,23,39,55,\n",
        "        8,24,40,56,9,25,41,57,10,26,42,58,11,27,43,59,\n",
        "        12,28,44,60,13,29,45,61,14,30,46,62,15,31,47,63]\n",
        "\n",
        "\n",
        "PBox_inv = [PBox.index(x) for x in range(64)]\n",
        "\n",
        "def generateRoundkeys80(key,rounds):\n",
        "        \"\"\"Generate the roundkeys for a 80-bit key\n",
        "\n",
        "        Input:\n",
        "                key:    the key as a 80-bit integer\n",
        "                rounds: the number of rounds as an integer\n",
        "        Output: list of 64-bit roundkeys as integers\"\"\"\n",
        "        roundkeys = []\n",
        "        for i in range(1,rounds+1): # (K1 ... K32)\n",
        "                # rawkey: used in comments to show what happens at bitlevel\n",
        "                # rawKey[0:64]\n",
        "                roundkeys.append(key >>16)\n",
        "                #1. Shift\n",
        "                #rawKey[19:len(rawKey)]+rawKey[0:19]\n",
        "                key = ((key & (2**19-1)) << 61) + (key >> 19)\n",
        "                #2. SBox\n",
        "                #rawKey[76:80] = S(rawKey[76:80])\n",
        "                key = (Sbox[key >> 76] << 76)+(key & (2**76-1))\n",
        "                #3. Salt\n",
        "                #rawKey[15:20] ^ i\n",
        "                key ^= i << 15\n",
        "        return roundkeys\n",
        "\n",
        "def generateRoundkeys128(key,rounds):\n",
        "        \"\"\"Generate the roundkeys for a 128-bit key\n",
        "\n",
        "        Input:\n",
        "                key:    the key as a 128-bit integer\n",
        "                rounds: the number of rounds as an integer\n",
        "        Output: list of 64-bit roundkeys as integers\"\"\"\n",
        "        roundkeys = []\n",
        "        for i in range(1,rounds+1): # (K1 ... K32)\n",
        "                # rawkey: used in comments to show what happens at bitlevel\n",
        "                roundkeys.append(key >>64)\n",
        "                #1. Shift\n",
        "                key = ((key & (2**67-1)) << 61) + (key >> 67)\n",
        "                #2. SBox\n",
        "                key = (Sbox[key >> 124] << 124)+(Sbox[(key >> 120) & 0xF] << 120)+(key & (2**120-1))\n",
        "                #3. Salt\n",
        "                #rawKey[62:67] ^ i\n",
        "                key ^= i << 62\n",
        "        return roundkeys\n",
        "\n",
        "def addRoundKey(state,roundkey):\n",
        "        return state ^ roundkey\n",
        "\n",
        "def sBoxLayer(state):\n",
        "        \"\"\"SBox function for encryption\n",
        "\n",
        "        Input:  64-bit integer\n",
        "        Output: 64-bit integer\"\"\"\n",
        "\n",
        "        output = 0\n",
        "        for i in range(16):\n",
        "                output += Sbox[( state >> (i*4)) & 0xF] << (i*4)\n",
        "        return output\n",
        "\n",
        "def sBoxLayer_dec(state):\n",
        "        \"\"\"Inverse SBox function for decryption\n",
        "\n",
        "        Input:  64-bit integer\n",
        "        Output: 64-bit integer\"\"\"\n",
        "        output = 0\n",
        "        for i in range(16):\n",
        "                output += Sbox_inv[( state >> (i*4)) & 0xF] << (i*4)\n",
        "        return output\n",
        "\n",
        "def pLayer(state):\n",
        "        \"\"\"Permutation layer for encryption\n",
        "\n",
        "        Input:  64-bit integer\n",
        "        Output: 64-bit integer\"\"\"\n",
        "        output = 0\n",
        "        for i in range(64):\n",
        "                output += ((state >> i) & 0x01) << PBox[i]\n",
        "        return output\n",
        "\n",
        "def pLayer_dec(state):\n",
        "        \"\"\"Permutation layer for decryption\n",
        "\n",
        "        Input:  64-bit integer\n",
        "        Output: 64-bit integer\"\"\"\n",
        "        output = 0\n",
        "        for i in range(64):\n",
        "                output += ((state >> i) & 0x01) << PBox_inv[i]\n",
        "        return output\n",
        "\n",
        "def string2number(i):\n",
        "    \"\"\" Convert a string to a number\n",
        "\n",
        "    Input: string (big-endian)\n",
        "    Output: long or integer\n",
        "    \"\"\"\n",
        "    return int(codecs.encode(i))\n",
        "    \n",
        "def number2string_N(i, N):\n",
        "    \"\"\"Convert a number to a string of fixed size\n",
        "\n",
        "    i: long or integer\n",
        "    N: length of string\n",
        "    Output: string (big-endian)\n",
        "    \"\"\"\n",
        "    s = '%0*x' % (N*2, i)\n",
        "    return codecs.decode(s,'hex')\n",
        "\n",
        "\n",
        "def bytes_to_int(bytes):\n",
        "        result = 0\n",
        "\n",
        "        for b in bytes:\n",
        "            result = result * 256 + int(b)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "def Binary(n):\n",
        "     s = bin(n)\n",
        "\n",
        "     # removing \"0b\" prefix\n",
        "     s1 = s[2:].zfill(64)\n",
        "     s2 = str(s1)\n",
        "     return s2\n",
        "\n",
        "def binaryToDecimal(n):\n",
        "    return int(n,2)\n",
        "\n",
        "import string\n",
        "\n",
        "indata = []\n",
        "outdata = []\n",
        "# here is the integer\n",
        "# print(bin(10018325639476897698)[2:])\n",
        "import random\n",
        "for i in range(10000):\n",
        "  h = random.getrandbits(64)\n",
        "  b = Binary(h)\n",
        "  indata.insert(i,b)\n",
        "  temp = ''\n",
        "  for j in range(2, 7):\n",
        "    data = Present(\"0100101001\", j)\n",
        "    k = data.encrypt(str(int(b,2)))\n",
        "    d = bytes_to_int(k)\n",
        "    a = Binary(d)\n",
        "    temp += a\n",
        "  outdata.insert(i,temp)\n",
        " # print(len(a))\n",
        "# # a is the output and is also 64 bits long\n",
        "# print(len(\"1000101100001000001111100001011011111001001101010110001001010101\"))\n",
        "# # print(len(Binary(bytes_to_int(k))))\n",
        "#\n",
        "# one part has been completed\n",
        "# we have 2 arrays both of size 10000 and both contain a bit sequence of 64 bits.\n",
        "import numpy as np\n",
        "inmat = np.empty((10000,80))\n",
        "outmat = np.empty((10000,64))\n",
        "\n",
        "for i in range(10000):\n",
        "  m = 0\n",
        "  n = 4\n",
        "  for k in range(80):\n",
        "    part = outdata[i][m:n]\n",
        "    part1 = binaryToDecimal(part)\n",
        "    inmat[i][k] = part1\n",
        "    m +=4\n",
        "    n +=4\n",
        "\n",
        "\n",
        "# for q in range(10000):\n",
        "# for w in range(16):\n",
        "# print(inmat[q][w])\n",
        "# for i in range(10000):\n",
        "#     print(outdata[i])\n",
        "\n",
        "for i in range(10000):\n",
        "  for k in range(64):\n",
        "    part = indata[i][k]\n",
        "    # part1 = binaryToDecimal(part)\n",
        "    if(part == ''):\n",
        "        continue\n",
        "    # print(part)\n",
        "    outmat[i][k] = part\n",
        "\n",
        "# for q in range(10000):\n",
        "#   for w in range(64):\n",
        "#      print(outmat[q][w])\n",
        "\n",
        "# now creating csv files\n",
        "import csv\n",
        "with open(\"Input_features.csv\",\"w+\") as my_csv:\n",
        "    csvWriter = csv.writer(my_csv,delimiter=',')\n",
        "    csvWriter.writerows(inmat)\n",
        "with open(\"Label.csv\",\"w+\") as my_csv:\n",
        "    csvWriter = csv.writer(my_csv,delimiter=',')\n",
        "    csvWriter.writerows(outmat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsVwuCU1OHsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(Z):\n",
        "  A = 1/(1+np.exp(-Z))\n",
        "  cache = Z\n",
        "  return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "  A = np.maximum(0,Z)\n",
        "  assert(A.shape == Z.shape)\n",
        "  cache = Z \n",
        "  return A, cache\n",
        "\n",
        "def derivative_sig(z):\n",
        "    return z * (1-z)\n",
        "\n",
        "def tanh(z):\n",
        "  a = np.exp(z)\n",
        "  b = np.exp(-z)\n",
        "  cache = z\n",
        "  return (a - b)/(a + b), cache\n",
        "\n",
        "def sigmoid_gradient(dA, Z):\n",
        "  A, Z = sigmoid(Z)\n",
        "  dZ = dA * A * (1 - A)\n",
        "\n",
        "  return dZ\n",
        "\n",
        "\n",
        "def tanh_gradient(dA, Z):\n",
        "  A, Z = tanh(Z)\n",
        "  dZ = dA * (1 - np.square(A))\n",
        "\n",
        "  return dZ\n",
        "\n",
        "\n",
        "def relu_gradient(dA, Z):\n",
        "  A, Z = relu(Z)\n",
        "  dZ = np.multiply(dA, np.int64(A > 0))\n",
        "\n",
        "  return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4X98y9FORS3",
        "colab_type": "code",
        "outputId": "5651515a-0182-4054-a35c-70b0f0975c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df=pd.read_csv('Input_features.csv')\n",
        "df1=pd.read_csv('Label.csv')\n",
        "\n",
        "X = df.iloc[:, 0:80].values\n",
        "Y = df1.iloc[:,0].values\n",
        "\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\n",
        "\n",
        "X_train=X_train/15\n",
        "X_test=X_test/15\n",
        "a1 = np.zeros(shape = (7999, 1))\n",
        "for i in range(7999):\n",
        "  a1[i][0] = Y_train[i]\n",
        "a1.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7999, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7Jd1CkgXKGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(layers_dims):\n",
        "\n",
        "    np.random.seed(1)               \n",
        "    parameters = {}\n",
        "    L = len(layers_dims)            \n",
        "\n",
        "    for l in range(1, L):           \n",
        "        parameters[\"W\" + str(l)] = np.random.randn(layers_dims[l - 1], layers_dims[l])\n",
        "        parameters[\"b\" + str(l)] = np.zeros((1, layers_dims[l]))\n",
        "\n",
        "        assert parameters[\"W\" + str(l)].shape == (layers_dims[l - 1], layers_dims[l])\n",
        "        assert parameters[\"b\" + str(l)].shape == (1, layers_dims[l])\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwgocNZUNERl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drop_out_matrices(layers_dims, m, keep_prob):\n",
        "\n",
        "  np.random.seed(1)\n",
        "  D = {}\n",
        "  L = len(layers_dims)\n",
        "\n",
        "  for l in range(1, L - 2):\n",
        "    D[str(l+1)] = np.random.rand(m, layers_dims[l])\n",
        "\n",
        "    for i in range(m):\n",
        "      for j in range(layers_dims[l]):\n",
        "        if D[str(l+1)][i][j] < keep_prob[l+1]:\n",
        "          D[str(l+1)][i][j] = 0\n",
        "        else:\n",
        "          D[str(l+1)][i][j] = 1\n",
        "\n",
        "    assert(D[str(l+1)]).shape == (m, layers_dims[l])\n",
        "\n",
        "  return D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYgzp_VXbfut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A_prev, W, b, dataset, l , p2, p3):\n",
        "  if dataset == \"Training\":\n",
        "    Z = np.dot(A_prev, W) + b\n",
        "    cache = (A_prev, W, b)\n",
        "  elif dataset == \"Testing\":\n",
        "    if l == 2:\n",
        "      Z = np.dot(A_prev, W*p2) + b\n",
        "      cache = (A_prev, W, b)\n",
        "    elif l == 3:\n",
        "      Z = np.dot(A_prev, W*p3) + b\n",
        "      cache = (A_prev, W, b)\n",
        "    else:\n",
        "      Z = np.dot(A_prev, W) + b\n",
        "      cache = (A_prev, W, b)\n",
        "      \n",
        "  return Z, cache\n",
        "\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, l, p2, p3, dataset, activation_fn):\n",
        "    assert activation_fn == \"sigmoid\" or activation_fn == \"tanh\" or activation_fn == \"relu\"\n",
        "\n",
        "    if activation_fn == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b, dataset, l, p2, p3)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "\n",
        "    elif activation_fn == \"tanh\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = tanh(Z)\n",
        "\n",
        "    elif activation_fn == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "\n",
        "    assert A.shape == (A_prev.shape[0], W.shape[1])\n",
        "\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def L_model_forward(X, parameters, D, keep_prob, p2, p3, hidden_layers_activation_fn, dataset):\n",
        "    A = X                           \n",
        "    caches = []                     \n",
        "    L = len(parameters) // 2   \n",
        "\n",
        "    for l in range(1, L):\n",
        "      A_prev = A\n",
        "\n",
        "      if dataset == \"Training\":\n",
        "        if l == 2 or l == 3:\n",
        "          A_prev = np.multiply(A_prev, D[str(l)])\n",
        "\n",
        "      A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], l, p2, p3, dataset, activation_fn = hidden_layers_activation_fn)\n",
        "      caches.append(cache)\n",
        "\n",
        "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], l, p2, p3, dataset, activation_fn=\"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    assert AL.shape == (X.shape[0], 1)\n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g62fRq5BM75x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backword(dZ, cache):\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[0]\n",
        "\n",
        "    dW = (1 / m) * np.dot(A_prev.T, dZ)\n",
        "    db = (1 / m) * np.sum(dZ, axis=0, keepdims=True)\n",
        "    dA_prev = np.dot(dZ, W.T)\n",
        "\n",
        "    assert dA_prev.shape == A_prev.shape\n",
        "    assert dW.shape == W.shape\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation_fn):\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if activation_fn == \"sigmoid\":\n",
        "        dZ = sigmoid_gradient(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n",
        "\n",
        "    elif activation_fn == \"tanh\":\n",
        "        dZ = tanh_gradient(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n",
        "\n",
        "    elif activation_fn == \"relu\":\n",
        "        dZ = relu_gradient(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "def L_model_backward(AL, y, caches, hidden_layers_activation_fn=\"relu\"):\n",
        "    L = len(caches)\n",
        "    grads = {}\n",
        "\n",
        "    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n",
        "\n",
        "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, caches[L - 1], \"sigmoid\")\n",
        "\n",
        "    for l in range(L - 1, 0, -1):\n",
        "      current_cache = caches[l - 1]\n",
        "      grads[\"dA\" + str(l - 1)], grads[\"dW\" + str(l)], grads[\"db\" + str(l)] = linear_activation_backward(grads[\"dA\" + str(l)], current_cache,hidden_layers_activation_fn)\n",
        "\n",
        "    return grads  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uslBNENuNsP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    for l in range(1, L + 1):\n",
        "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
        "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmA7qeGL6y0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, y):\n",
        "  m = y.shape[0]              \n",
        "  cost = - (1 / m) * np.sum(np.multiply(y, np.log(AL)) + np.multiply(1 - y, np.log(1 - AL)))\n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STkdQMCAcqt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_layer_model(X, y, layers_dims, learning_rate=0.01, num_iterations=5, print_cost=True, hidden_layers_activation_fn=\"sigmoid\"):\n",
        "\n",
        "    np.random.seed(1)\n",
        "    keep_prob = [1, 1, 0.5, 0.75, 1]\n",
        "    p2 = keep_prob[2]\n",
        "    p3 = keep_prob[3]\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "    costs = []\n",
        "    iters = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "      D = drop_out_matrices(layers_dims, 7999, keep_prob)\n",
        "      AL, caches = L_model_forward(X, parameters, D, keep_prob, p2, p3, hidden_layers_activation_fn, dataset = \"Training\")\n",
        "      cost = compute_cost(AL, y)\n",
        "      print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "      costs.append(cost)\n",
        "      iters.append(i )\n",
        "      grads = L_model_backward(AL, y, caches, hidden_layers_activation_fn)\n",
        "      parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "    plt.plot(iters, costs)\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    layer_2, caches = L_model_forward(X_test, parameters, D, keep_prob, p2, p3, hidden_layers_activation_fn, dataset = \"Testing\")\n",
        "    for i in range(0,40):\n",
        "      print(layer_2[i],Y_test[i])\n",
        "    a = 0\n",
        "    for i in range(0,2000):\n",
        "      if layer_2[i] <= 0.50 and Y_test[i] == 0 :\n",
        "        a += 1\n",
        "      elif layer_2[i] > 0.50 and Y_test[i] == 1 :\n",
        "        a += 1\n",
        "    print((a/2000)*100)\n",
        "    return parameters  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXbyuSFTdVut",
        "colab_type": "code",
        "outputId": "d352cbc0-ed2b-4d2b-f011-b054a2ef86f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "layers_dims = [X_train.shape[1], 16, 64, 18, 1]\n",
        "parameters_tanh = L_layer_model( X_train, a1, layers_dims, learning_rate=0.03, num_iterations=100, hidden_layers_activation_fn=\"sigmoid\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 2.181918\n",
            "Cost after iteration 1: 2.132415\n",
            "Cost after iteration 2: 2.083567\n",
            "Cost after iteration 3: 2.035407\n",
            "Cost after iteration 4: 1.987967\n",
            "Cost after iteration 5: 1.941284\n",
            "Cost after iteration 6: 1.895391\n",
            "Cost after iteration 7: 1.850325\n",
            "Cost after iteration 8: 1.806122\n",
            "Cost after iteration 9: 1.762818\n",
            "Cost after iteration 10: 1.720449\n",
            "Cost after iteration 11: 1.679049\n",
            "Cost after iteration 12: 1.638653\n",
            "Cost after iteration 13: 1.599293\n",
            "Cost after iteration 14: 1.561000\n",
            "Cost after iteration 15: 1.523802\n",
            "Cost after iteration 16: 1.487724\n",
            "Cost after iteration 17: 1.452791\n",
            "Cost after iteration 18: 1.419021\n",
            "Cost after iteration 19: 1.386430\n",
            "Cost after iteration 20: 1.355032\n",
            "Cost after iteration 21: 1.324834\n",
            "Cost after iteration 22: 1.295840\n",
            "Cost after iteration 23: 1.268051\n",
            "Cost after iteration 24: 1.241462\n",
            "Cost after iteration 25: 1.216065\n",
            "Cost after iteration 26: 1.191848\n",
            "Cost after iteration 27: 1.168794\n",
            "Cost after iteration 28: 1.146884\n",
            "Cost after iteration 29: 1.126094\n",
            "Cost after iteration 30: 1.106398\n",
            "Cost after iteration 31: 1.087767\n",
            "Cost after iteration 32: 1.070169\n",
            "Cost after iteration 33: 1.053571\n",
            "Cost after iteration 34: 1.037937\n",
            "Cost after iteration 35: 1.023230\n",
            "Cost after iteration 36: 1.009413\n",
            "Cost after iteration 37: 0.996447\n",
            "Cost after iteration 38: 0.984293\n",
            "Cost after iteration 39: 0.972913\n",
            "Cost after iteration 40: 0.962269\n",
            "Cost after iteration 41: 0.952321\n",
            "Cost after iteration 42: 0.943032\n",
            "Cost after iteration 43: 0.934366\n",
            "Cost after iteration 44: 0.926288\n",
            "Cost after iteration 45: 0.918761\n",
            "Cost after iteration 46: 0.911753\n",
            "Cost after iteration 47: 0.905233\n",
            "Cost after iteration 48: 0.899168\n",
            "Cost after iteration 49: 0.893530\n",
            "Cost after iteration 50: 0.888291\n",
            "Cost after iteration 51: 0.883423\n",
            "Cost after iteration 52: 0.878903\n",
            "Cost after iteration 53: 0.874706\n",
            "Cost after iteration 54: 0.870809\n",
            "Cost after iteration 55: 0.867192\n",
            "Cost after iteration 56: 0.863834\n",
            "Cost after iteration 57: 0.860718\n",
            "Cost after iteration 58: 0.857825\n",
            "Cost after iteration 59: 0.855140\n",
            "Cost after iteration 60: 0.852646\n",
            "Cost after iteration 61: 0.850330\n",
            "Cost after iteration 62: 0.848179\n",
            "Cost after iteration 63: 0.846179\n",
            "Cost after iteration 64: 0.844321\n",
            "Cost after iteration 65: 0.842593\n",
            "Cost after iteration 66: 0.840984\n",
            "Cost after iteration 67: 0.839487\n",
            "Cost after iteration 68: 0.838092\n",
            "Cost after iteration 69: 0.836792\n",
            "Cost after iteration 70: 0.835579\n",
            "Cost after iteration 71: 0.834446\n",
            "Cost after iteration 72: 0.833388\n",
            "Cost after iteration 73: 0.832398\n",
            "Cost after iteration 74: 0.831471\n",
            "Cost after iteration 75: 0.830603\n",
            "Cost after iteration 76: 0.829788\n",
            "Cost after iteration 77: 0.829022\n",
            "Cost after iteration 78: 0.828302\n",
            "Cost after iteration 79: 0.827624\n",
            "Cost after iteration 80: 0.826985\n",
            "Cost after iteration 81: 0.826381\n",
            "Cost after iteration 82: 0.825810\n",
            "Cost after iteration 83: 0.825270\n",
            "Cost after iteration 84: 0.824757\n",
            "Cost after iteration 85: 0.824270\n",
            "Cost after iteration 86: 0.823806\n",
            "Cost after iteration 87: 0.823365\n",
            "Cost after iteration 88: 0.822943\n",
            "Cost after iteration 89: 0.822540\n",
            "Cost after iteration 90: 0.822153\n",
            "Cost after iteration 91: 0.821783\n",
            "Cost after iteration 92: 0.821427\n",
            "Cost after iteration 93: 0.821084\n",
            "Cost after iteration 94: 0.820753\n",
            "Cost after iteration 95: 0.820434\n",
            "Cost after iteration 96: 0.820125\n",
            "Cost after iteration 97: 0.819826\n",
            "Cost after iteration 98: 0.819535\n",
            "Cost after iteration 99: 0.819253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJOYUBCIiMacgEBGJuf8PfPeJ3kl+wTUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0.66417006] 0.0\n",
            "[0.6035087] 1.0\n",
            "[0.63444] 0.0\n",
            "[0.62706807] 0.0\n",
            "[0.67680175] 1.0\n",
            "[0.64922224] 0.0\n",
            "[0.6316712] 1.0\n",
            "[0.64814348] 0.0\n",
            "[0.67285701] 0.0\n",
            "[0.64614648] 1.0\n",
            "[0.55983385] 1.0\n",
            "[0.66876536] 0.0\n",
            "[0.66604775] 1.0\n",
            "[0.67839424] 0.0\n",
            "[0.59458904] 1.0\n",
            "[0.63540427] 0.0\n",
            "[0.68793204] 0.0\n",
            "[0.65846486] 1.0\n",
            "[0.60398813] 1.0\n",
            "[0.61869216] 0.0\n",
            "[0.68556279] 0.0\n",
            "[0.65634784] 1.0\n",
            "[0.64547993] 0.0\n",
            "[0.62474696] 0.0\n",
            "[0.61194168] 0.0\n",
            "[0.6051889] 0.0\n",
            "[0.64600387] 1.0\n",
            "[0.63680209] 1.0\n",
            "[0.56458563] 1.0\n",
            "[0.57657463] 1.0\n",
            "[0.66843679] 1.0\n",
            "[0.63156997] 0.0\n",
            "[0.64072272] 1.0\n",
            "[0.66170598] 1.0\n",
            "[0.61949694] 1.0\n",
            "[0.61984086] 1.0\n",
            "[0.62388673] 0.0\n",
            "[0.61638463] 0.0\n",
            "[0.65420052] 0.0\n",
            "[0.61215989] 1.0\n",
            "51.24999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOc6f2uBO3f9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,40):\n",
        "   print(layer_2[i],Y_test[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxG58KH8eNX-",
        "colab_type": "code",
        "outputId": "a8a86b94-d94c-4aaf-b61a-75912667ce88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "a = 0\n",
        "for i in range(0,2000):\n",
        "  if layer_2[i] <= 0.50 and Y_test[i] == 0 :\n",
        "    a += 1\n",
        "  elif layer_2[i] > 0.50 and Y_test[i] == 1 :\n",
        "    a += 1\n",
        "print((a/2000)*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-61d53d07e8b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mlayer_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.50\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlayer_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.50\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'layer_2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AX6RhFceVIx",
        "colab_type": "code",
        "outputId": "15f4fa80-6135-45b8-e24c-aa7b047bba3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "type(np.random.rand(3,3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdKRdg9sP0n_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}